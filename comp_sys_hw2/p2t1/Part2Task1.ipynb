{"cells":[{"cell_type":"code","execution_count":1,"id":"7411c8a2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/04/29 19:29:23 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/04/29 19:29:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/04/29 19:29:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/04/29 19:29:23 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/04/29 19:29:30 WARN org.apache.spark.streaming.StreamingContext: Dynamic Allocation is enabled for this application. Enabling Dynamic allocation for Spark Streaming applications can cause data loss if Write Ahead Log is not enabled for non-replayable sources. See the programming guide for details on how to enable the Write Ahead Log.\n","22/04/29 19:30:10 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/04/29 19:30:10 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/04/29 19:30:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (cluster-19bf-w-1.c.probable-skill-340100.internal executor 1): java.io.FileNotFoundException: File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n","\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n","\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:871)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:858)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:847)\n","\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1015)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:322)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:318)\n","\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:330)\n","\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:907)\n","\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:86)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:216)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:213)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:168)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1572)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1518)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n","\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:327)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n","\tat com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:869)\n","\t... 32 more\n","\n","22/04/29 19:30:15 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n","22/04/29 19:30:15 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651260610000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_15463/867885702.py\", line 27, in <lambda>\n","    stream_data.foreachRDD( lambda rdd: readMyStream(rdd) )\n","  File \"/tmp/ipykernel_15463/867885702.py\", line 18, in readMyStream\n","    if not rdd.isEmpty():\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1606, in isEmpty\n","    return self.getNumPartitions() == 0 or len(self.take(1)) == 0\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1566, in take\n","    res = self.context.runJob(self, takeUpToNumLeft, p)\n","  File \"/usr/lib/spark/python/pyspark/context.py\", line 1233, in runJob\n","    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (cluster-19bf-w-0.c.probable-skill-340100.internal executor 2): java.io.FileNotFoundException: File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n","\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n","\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:871)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:858)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:847)\n","\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1015)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:322)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:318)\n","\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:330)\n","\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:907)\n","\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:86)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:216)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:213)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:168)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1572)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1518)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n","\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:327)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n","\tat com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:869)\n","\t... 32 more\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n","\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n","\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: java.io.FileNotFoundException: File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n","\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n","\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:871)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:858)\n","\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:847)\n","\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1015)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:322)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:318)\n","\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n","\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:330)\n","\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:907)\n","\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:86)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:216)\n","\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:213)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:168)\n","\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\t... 1 more\n","Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /test/part-00000-32b62054-76cc-48d8-abe1-34915e220453-c000.csv._COPYING_\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n","\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n","\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1990)\n","\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:768)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:442)\n","\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\n","\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\n","\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\n","\tat java.security.AccessController.doPrivileged(Native Method)\n","\tat javax.security.auth.Subject.doAs(Subject.java:422)\n","\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n","\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\n","\n","\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1572)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1518)\n","\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n","\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\n","\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:327)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n","\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n","\tat com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source)\n","\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:869)\n","\t... 32 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Started the Process\n","Selection of Columns\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+------------------+------------------+\n","|               _c0|               _c1|\n","+------------------+------------------+\n","|                 !|0.8118955471038851|\n","|                 !|0.8118955471038853|\n","|         ! (album)|1.8177773796591234|\n","|               !!!| 4.586973636963882|\n","|    !!!fuck you!!!|0.8009858772095582|\n","|    !!!fuck you!!!|0.8009858772095583|\n","|!!destroy-oh-boy!!|0.7825583571751865|\n","|!!destroy-oh-boy!!|0.7825583571751866|\n","|             !hero|0.8752332490116643|\n","|             !hero|0.8752332490116644|\n","|               !k7|0.9934762717399955|\n","|               !k7|0.9934762717399956|\n","|       !k7 records| 1.017743036277492|\n","|       !k7 records|1.0177430362774922|\n","|    !kung language|1.8145324402362248|\n","|    !kung language|1.8145324402362253|\n","|      !kung people|1.2548089190492362|\n","|        !t.o.o.h.!|1.5280151252107748|\n","|        !t.o.o.h.!| 1.528015125210775|\n","|              !xóõ|1.7717834415072888|\n","+------------------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from pyspark.streaming import StreamingContext\n","from pyspark.sql.types import IntegerType, LongType, DecimalType,StructType, StructField, StringType\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col\n","import pyspark.sql.functions as F\n","from pyspark.sql import Window\n","\n","sc = SparkContext.getOrCreate()\n","spark = SparkSession(sc)\n","ssc = StreamingContext(sc, 5)\n","\n","stream_data = ssc.textFileStream(\"/test/\")\n","\n","\n","def readMyStream(rdd):\n","    if not rdd.isEmpty():\n","        df = spark.read.option(\"delimiter\", \"\\t\").csv(rdd)\n","        print('Started the Process')\n","        print('Selection of Columns')\n","        df = df.filter(df._c1 > 0.5)\n","        #df = df.select('t1','t2','t3','timestamp').where(col(\"timestamp\").isNotNull())\n","        df.show()\n","        df.coalesce(1).write.option(\"delimiter\", \"\\t\").csv(\"/datacsv\")\n","\n","stream_data.foreachRDD( lambda rdd: readMyStream(rdd) )\n","ssc.start()\n","#ssc.stop()"]},{"cell_type":"code","execution_count":5,"id":"b3c24f84","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/04/29 19:46:58 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/04/29 19:46:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/04/29 19:46:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/04/29 19:46:58 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","[Stage 1:=============================>                             (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["2675032\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["spark = SparkSession.builder.getOrCreate()\n","df = spark.read.option(\"delimiter\", \"\\t\").csv('/datacsv/part-00000-1cc56b38-87cf-4848-995f-4f38881c5839-c000.csv')\n","print(df.count())"]},{"cell_type":"code","execution_count":2,"id":"2d94e765","metadata":{},"outputs":[],"source":["ssc.stop()"]},{"cell_type":"code","execution_count":null,"id":"f5291f5d","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}